{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Starter code for Cloud Classification Challenge\n",
    "\n",
    "This code is designed as starter point for your development. You do not have to use it, but feel free to use it if you do not know where to start.\n",
    "\n",
    "The [Pytorch](https://pytorch.org/) collection of packages is used to define and train the model, and this code is adapted from their [introductory tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html).\n",
    "\n",
    "Other machine learning python packages that you may wish to use include [TensorFlow](https://www.tensorflow.org/overview) and [scikit-learn](https://scikit-learn.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Dataset for sat images\n",
    "\n",
    "Dataset instance reads in the directory to the images and their labels.\n",
    "The dataloader enables simple iteration over these images when training and testing a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for label data\n",
    "def get_label_dict():\n",
    "    label_dict = {\"Fish\": 0,\n",
    "                  \"Flower\": 1,\n",
    "                  \"Gravel\": 2,\n",
    "                  \"Sugar\": 3}\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def sat_label_transform(label):\n",
    "    label_dict = get_label_dict()\n",
    "    return label_dict[label]\n",
    "\n",
    "\n",
    "def sat_label_transform_inv(num):\n",
    "    label_dict = get_label_dict()\n",
    "    ret_list = [key for key in label_dict.keys() if label_dict[key]==num]\n",
    "    return ret_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform for images.\n",
    "# Converts to float and scales values to range 0-1.\n",
    "# Normalisation using the mean/std used by AlexNet.\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for loading the satellite image into a Dataset\n",
    "class SatImageDataset(Dataset):\n",
    "    def __init__(self, labels_file, img_dir, transform=img_transform, target_transform=sat_label_transform):\n",
    "        self.img_labels = pd.read_csv(labels_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[\"Image\"].iloc[idx])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels[\"Label\"].iloc[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and testing data using instances of the SatImageDataset defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data.\n",
    "train_files_dir = \"/data/users/meastman/understanding_clouds_kaggle/input/single_labels/224s/train/\"\n",
    "train_files_labels = \"/data/users/meastman/understanding_clouds_kaggle/input/single_labels/224s/train/train_labels.csv\"\n",
    "\n",
    "# Create train images dataloader\n",
    "train_images = SatImageDataset(labels_file=train_files_labels, img_dir=train_files_dir)\n",
    "train_dataloader = DataLoader(train_images, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "test_files_dir = \"/data/users/meastman/understanding_clouds_kaggle/input/single_labels/224s/test/\"\n",
    "test_files_labels = \"/data/users/meastman/understanding_clouds_kaggle/input/single_labels/224s/test/test_labels.csv\"\n",
    "\n",
    "# Create test images dataloader\n",
    "test_images = SatImageDataset(labels_file=test_files_labels, img_dir=test_files_dir)\n",
    "test_dataloader = DataLoader(test_images, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network\n",
    "\n",
    "This is a fully connected neural network with 3 hidden layers. For more details on the individual layers, and for further options if you wish to create a different model architecture see [the tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html).\n",
    "\n",
    "Note that the input to the layer has size `150528 = 3*224*224`. The input images are 224 * 224 pixels, with 3 RGB channels.\n",
    "\n",
    "The output layer has size 4 which matches the number of cloud categories available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3*224*224, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which hardware is available so we set up the model correctly.\n",
    "VDI/SPICE are CPU, but GPUs give better performance for training if available.\n",
    "\n",
    "If you have the time/inclination/cost code GPUs are available through [AWS self service](https://web.yammer.com/main/org/metoffice.gov.uk/threads/eyJfdHlwZSI6IlRocmVhZCIsImlkIjoiMjM4MjQzNTE0MTIzMDU5MiJ9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=150528, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the loss function for our model to compare it's outputs to the true values. This allows the parameters (the weights and biases of the neural network) to be incrementally improved as the model sees more data.\n",
    "\n",
    "As we have a classification task with multiple categories use the [Cross Entropy Loss](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model and find the best parameters we want to minimise the loss function use Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate controls how quickly the model converges.\n",
    "learning_rate = 1e-3\n",
    "# How many images given to the model at each time\n",
    "batch_size = 32\n",
    "# How many loops through the data. \n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some functions for training the model and testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the model\n",
    "\n",
    "Now train and test the model!\n",
    "\n",
    "N.B. This step will probably take a long time depending on your computing resources available.\n",
    "It took me about 80 minutes for 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------\n",
      "loss: 1.403055  [   32/69952]\n",
      "loss: 1.278762  [ 3232/69952]\n",
      "loss: 1.269672  [ 6432/69952]\n",
      "loss: 1.229406  [ 9632/69952]\n",
      "loss: 1.160386  [12832/69952]\n",
      "loss: 1.186415  [16032/69952]\n",
      "loss: 1.231033  [19232/69952]\n",
      "loss: 1.089999  [22432/69952]\n",
      "loss: 1.028146  [25632/69952]\n",
      "loss: 1.079422  [28832/69952]\n",
      "loss: 1.160247  [32032/69952]\n",
      "loss: 1.081921  [35232/69952]\n",
      "loss: 1.267946  [38432/69952]\n",
      "loss: 1.343846  [41632/69952]\n",
      "loss: 1.331972  [44832/69952]\n",
      "loss: 0.941176  [48032/69952]\n",
      "loss: 1.138247  [51232/69952]\n",
      "loss: 1.216028  [54432/69952]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn,optimizer)\n",
    "    test_loop(train_dataloader, model, loss_fn)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output\n",
    "\n",
    "Check the output of the model to find it's predictions for a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch of images and labels\n",
    "t_imgs, t_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn the output into a prediction, first convert it to probabilities using the [softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax) function.\n",
    "\n",
    "Then select the category with the highest probability as the model prediction.\n",
    "Convert this back into the text label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 3, 2, 0, 3, 0, 3, 3, 2, 2, 0, 2, 0, 2, 0, 0, 0, 0, 3, 1, 0, 2, 3,\n",
       "        3, 3, 0, 0, 3, 1, 3, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(t_imgs)\n",
    "probs = torch.nn.Softmax(dim=1)\n",
    "preds = probs.argmax(1)\n",
    "pred_labels = [sat_label_transform_inv(pred) for pred in preds]\n",
    "\n",
    "print(\"Predicted Labels\\n\", pred_labels)\n",
    "print(\"True Labels\\n\", t_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-dscop_cloud_class Python (Conda)",
   "language": "python",
   "name": "conda-env-.conda-dscop_cloud_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
