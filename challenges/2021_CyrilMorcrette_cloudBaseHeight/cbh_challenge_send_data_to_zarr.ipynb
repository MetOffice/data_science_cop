{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6729aad1-261a-48fa-ae59-986428165ebd",
   "metadata": {},
   "source": [
    "# Conversion of Data to a Machine Learning Friendly Format\n",
    "\n",
    "This notebook demonstrates taking a single NetCDF file and converting the file into analysis ready numpy arrays stored in a zarr file for later use in neural network training.\n",
    "\n",
    "Specifically, after having loaded multiple NetCDF files from UM model data into a single Iris CubeList, and saving this CubeList to disk, this notebook will:<ul>\n",
    "\n",
    "<li>Load the single NetCDF file back from disk.</li>\n",
    "<li>Extract the desired cubes: cloud volume fraction, specific humidity, air pressure, and air temperature.</li>\n",
    "<li>Combine cubes of the same feature where metadata differences have prevented concatenation.</li>\n",
    "<li>Convert the cubes to numpy arrays.</li>\n",
    "<li>Format the arrays into a desirable dimension: (Sample Number, Height Level, Feature).</li>\n",
    "<li>Generate data for the desired target we want to make a prediction on (cloud base height at a level in a sample).</li>\n",
    "<li>Normalize data where necessary.</li>\n",
    "<li>Save the data to disk for later loading to perform ML tasks.</li></ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b11403-49b8-413b-979d-e38b6d119b1e",
   "metadata": {},
   "source": [
    "Define imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4cd0c5-8ca7-47ac-bd69-7c717bc08782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h02/hsouth/.conda/envs/large/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "import dask\n",
    "import iris\n",
    "import numpy as np\n",
    "\n",
    "import cbh_data_definitions #  used for testing the load back in of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904eeb91-a733-4323-a0f3-13a3921f6e0f",
   "metadata": {},
   "source": [
    "Define file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b58e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_directory = pathlib.Path(os.environ[\"SCRATCH\"]) / \"cbh_data\"\n",
    "\n",
    "paths_to_load = (\n",
    "    root_data_directory / \"train\" / \"train_large.nc\"\n",
    ")  # one large nc file of iris' concatenation of all small nc files\n",
    "path_to_save_result = (\n",
    "    root_data_directory / \"analysis_ready\" / \"example_train.npz\"\n",
    ")  # ouput for numpy arrays\n",
    "path_to_save_zarr = (\n",
    "    root_data_directory / \"analysis_ready\" / \"example_train.zarr\"\n",
    ")  # output for zarr files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824aae9-f9ca-4029-85d7-488c0f5b3fab",
   "metadata": {},
   "source": [
    "Settings for the notebook, with each constant given a comment above for a purpose description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44163e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a positional encoding array for a feature of each height layer in the sample\n",
    "GENERATE_POSITIONAL_ENCODING_ARRAYS = False\n",
    "# adds the height layer number to every feature vector in the input array\n",
    "if GENERATE_POSITIONAL_ENCODING_ARRAYS:\n",
    "    CONCATENATE_POSITIONAL_ENCONDING_TO_FEATURE_VECTOR = False\n",
    "\n",
    "# realises the input array computation in two halves to avoid memory constraints of large computation\n",
    "COMPUTE_INPUT_ARRAY_IN_HALVES = False\n",
    "\n",
    "FREE_UP_MEMORY_AFTER_TARGET_COMPUTATION = False\n",
    "\n",
    "# show all samples where clouds exist in the final layer (none)\n",
    "# the final layer is used as the desired classification in the case of no cloud base existance prediction\n",
    "VERIFY_NO_FINAL_LAYER_CLOUDS = False\n",
    "\n",
    "# do extra compute to find the number of samples with cloud bases in the dataset\n",
    "COMPUTE_CLOUD_BASE_SAMPLE_NUMBER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a123c-373b-4b4a-b195-9be635843400",
   "metadata": {},
   "source": [
    "## Loading in the Cloud Base Height Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e77b6bd-0989-4b9a-b7df-962ec9a2cb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find files complete, list of paths: /scratch/hsouth/cbh_data/train/train_large.nc\n"
     ]
    }
   ],
   "source": [
    "cubes = iris.load(str(paths_to_load))\n",
    "\n",
    "print(\"Find files complete, list of paths:\", paths_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab36fc-0c4f-4e86-8e89-e1da1474c74f",
   "metadata": {},
   "source": [
    "Show cube names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa75b77f-00c1-4437-b0d4-17740d621f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cube names:\n",
      " ['cloud_volume_fraction_in_atmosphere_layer', 'm01s05i250_0', 'cloud_volume_fraction_in_atmosphere_layer', 'm01s05i250', 'air_pressure', 'air_pressure', 'air_temperature', 'air_temperature', 'convective_rainfall_flux', 'convective_rainfall_flux', 'convective_snowfall_flux', 'convective_snowfall_flux', 'specific_humidity', 'specific_humidity', 'stratiform_rainfall_flux', 'stratiform_rainfall_flux', 'stratiform_snowfall_flux', 'stratiform_snowfall_flux', 'upward_air_velocity', 'upward_air_velocity']\n",
      "\n",
      "Example of cube metadata: cloud_volume_fraction_in_atmosphere_layer / (1) (forecast_period: 4; forecast_reference_time: 31; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "    Dimension coordinates:\n",
      "        forecast_period                                         x                           -                       -             -               -\n",
      "        forecast_reference_time                                 -                           x                       -             -               -\n",
      "        model_level_number                                      -                           -                       x             -               -\n",
      "        latitude                                                -                           -                       -             x               -\n",
      "        longitude                                               -                           -                       -             -               x\n",
      "    Auxiliary coordinates:\n",
      "        time                                                    x                           x                       -             -               -\n",
      "        level_height                                            -                           -                       x             -               -\n",
      "        sigma                                                   -                           -                       x             -               -\n",
      "    Attributes:\n",
      "        Conventions                             'CF-1.7'\n",
      "        STASH                                   m01s00i266\n",
      "        source                                  'Data from Met Office Unified Model'\n",
      "        um_version                              '10.9'\n"
     ]
    }
   ],
   "source": [
    "print(\"Cube names:\\n\", [str(cube.name()) for cube in cubes])\n",
    "\n",
    "print(\"\\n\" + \"Example of cube metadata:\", cubes[2].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da491ed-9be1-41be-9d6a-1ddb7a8089d8",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f04c28-c350-4a17-9332-6e1e78d0366e",
   "metadata": {},
   "source": [
    "### Extract the desired cubes: cloud volume fraction, specific humidity, air pressure, and air temperature\n",
    "\n",
    "Cloud volume fraction will be used as our target for the problem, and the rest of the cubes are used as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f64fe0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(cubes):\n",
    "    list_of_input_cubes = [\"air_temperature\", \"air_pressure\", \"specific_humidity\"]\n",
    "    target_cube_name = [\"cloud_volume_fraction_in_atmosphere_layer\"]\n",
    "\n",
    "    target_cube = iris.cube.CubeList(\n",
    "        [cube for cube in cubes if (cube.long_name in target_cube_name)]\n",
    "    )\n",
    "    inp_cube = iris.cube.CubeList(\n",
    "        [cube for cube in cubes if (cube.standard_name in list_of_input_cubes)]\n",
    "    )\n",
    "\n",
    "    return inp_cube, target_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ab1af-b134-4222-92bf-9b6110f0abfc",
   "metadata": {},
   "source": [
    "Call the function defined above and verify success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf809cd-a16a-4a80-a5f7-6311f89ef1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input cube:\n",
      " 0: air_pressure / (Pa)                 (forecast_period: 4; forecast_reference_time: 60; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "1: air_pressure / (Pa)                 (forecast_period: 4; forecast_reference_time: 31; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "2: air_temperature / (K)               (forecast_period: 4; forecast_reference_time: 31; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "3: air_temperature / (K)               (forecast_period: 4; forecast_reference_time: 60; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "4: specific_humidity / (kg kg-1)       (forecast_period: 4; forecast_reference_time: 60; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "5: specific_humidity / (kg kg-1)       (forecast_period: 4; forecast_reference_time: 31; model_level_number: 70; latitude: 480; longitude: 640) \n",
      "\n",
      "target cubes:\n",
      " 0: cloud_volume_fraction_in_atmosphere_layer / (1) (forecast_period: 4; forecast_reference_time: 60; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "1: cloud_volume_fraction_in_atmosphere_layer / (1) (forecast_period: 4; forecast_reference_time: 31; model_level_number: 70; latitude: 480; longitude: 640)\n"
     ]
    }
   ],
   "source": [
    "inp_cube, tar_cube = create_dataset(cubes)\n",
    "\n",
    "print(\"input cube:\\n\", inp_cube, \"\\n\")\n",
    "print(\"target cubes:\\n\", tar_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ade2f7-717b-4fd1-aca2-74f230dcef4b",
   "metadata": {},
   "source": [
    "### Combine cubes of the same feature where metadata differences have prevented concatenation, while also extracting the numpy array of each cube\n",
    "\n",
    "if duplicate cubes exist, concatenate them using numpy to avoid metadata matching issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4dacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_same_cubes(cube_list):\n",
    "\n",
    "    cube_name_dictionary = {}\n",
    "\n",
    "    for cube in cube_list:\n",
    "        # print('start cube load')\n",
    "        cube_np_array = cube.core_data()\n",
    "        # print('end load')\n",
    "\n",
    "        if not cube.long_name is None:\n",
    "            cube_name = cube.long_name\n",
    "        elif not cube.standard_name is None:\n",
    "            cube_name = cube.standard_name\n",
    "        else:\n",
    "            raise Exception(\"No name found on cube\")\n",
    "\n",
    "        try:\n",
    "            # concat along the differing axis, forcast reference time\n",
    "            cube_name_dictionary[cube_name] = np.concatenate(\n",
    "                (cube_np_array, cube_name_dictionary[cube_name]), axis=1\n",
    "            )\n",
    "\n",
    "            # print(cube_name_dictionary[cube_name].shape)\n",
    "\n",
    "        except KeyError:\n",
    "            cube_name_dictionary[cube_name] = cube_np_array\n",
    "\n",
    "    return cube_name_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f9fbd-3e2f-4f7e-ac68-6ad93fc207f8",
   "metadata": {},
   "source": [
    "Call the function defined above and verify success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a8aa14c-8721-4f75-866d-1e6b6bb941c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air Pressure array shape: (4, 91, 70, 480, 640)\n",
      "Cloud Volume array shape: (4, 91, 70, 480, 640)\n",
      "Array types: <class 'dask.array.core.Array'>\n"
     ]
    }
   ],
   "source": [
    "inp_dict = concatenate_same_cubes(inp_cube)\n",
    "tar_dict = concatenate_same_cubes(tar_cube)\n",
    "\n",
    "print(\"Air Pressure array shape:\", inp_dict[\"air_pressure\"].shape)\n",
    "print(\n",
    "    \"Cloud Volume array shape:\",\n",
    "    tar_dict[\"cloud_volume_fraction_in_atmosphere_layer\"].shape,\n",
    ")\n",
    "print(\"Array types:\", type(inp_dict[\"air_pressure\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98095b-7b0f-4a0f-ac91-c356a807d8cb",
   "metadata": {},
   "source": [
    "Combine dictionary elements to one array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99c38b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feats(dict_of_feats):\n",
    "\n",
    "    add_dim_for_feature = [np.expand_dims(x, axis=0) for x in dict_of_feats.values()]\n",
    "    feat_concat_array = np.concatenate(add_dim_for_feature, axis=0)\n",
    "    return feat_concat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "807eccb8-714e-48bf-9718-40ed3391b611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions to standardize for processing:\n",
      "Current Input Shape: (3, 4, 91, 70, 480, 640)\n",
      "Current Target Shape: (1, 4, 91, 70, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "inp_array = combine_feats(inp_dict)\n",
    "tar_array = combine_feats(tar_dict)\n",
    "\n",
    "# verify and check dims\n",
    "print(\"Dimensions to standardize for processing:\")\n",
    "print(\"Current Input Shape:\", inp_array.shape)\n",
    "print(\"Current Target Shape:\", tar_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7139f-fa77-4fad-934c-f5be8f866cfe",
   "metadata": {},
   "source": [
    "Expand the dimensions of 'short' arrays to work in flattening (this applies in practice the smaller dev set of the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a2401c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(inp_array.shape) == 4:\n",
    "    time_time2_dims_to_add = [1, 2]\n",
    "    inp_array = np.expand_dims(inp_array, time_time2_dims_to_add)\n",
    "    tar_array = np.expand_dims(tar_array, time_time2_dims_to_add)\n",
    "    print(\"New and correct shapes (should be 6 dims):\")\n",
    "    print(inp_array.shape)\n",
    "    print(tar_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b5a995e-b09f-4f52-a9f0-605071b9afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show array storage metadata:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 87.48 GiB </td>\n",
       "                        <td> 82.03 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (3, 4, 91, 70, 480, 640) </td>\n",
       "                        <td> (1, 1, 1, 70, 480, 640) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 19 Graph Layers </td>\n",
       "                        <td> 1092 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float32 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"462\" height=\"162\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"24\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"6\" x2=\"24\" y2=\"21\" />\n",
       "  <line x1=\"10\" y1=\"12\" x2=\"24\" y2=\"27\" />\n",
       "  <line x1=\"10\" y1=\"19\" x2=\"24\" y2=\"34\" />\n",
       "  <line x1=\"10\" y1=\"25\" x2=\"24\" y2=\"40\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"14\" y2=\"30\" />\n",
       "  <line x1=\"19\" y1=\"9\" x2=\"19\" y2=\"35\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"40\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 24.9485979497544,14.948597949754403 24.9485979497544,40.36121446433689 10.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"50\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"55\" y2=\"4\" />\n",
       "  <line x1=\"19\" y1=\"9\" x2=\"60\" y2=\"9\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"65\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"24\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"25\" y2=\"14\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"27\" y2=\"14\" />\n",
       "  <line x1=\"13\" y1=\"0\" x2=\"28\" y2=\"14\" />\n",
       "  <line x1=\"14\" y1=\"0\" x2=\"29\" y2=\"14\" />\n",
       "  <line x1=\"16\" y1=\"0\" x2=\"31\" y2=\"14\" />\n",
       "  <line x1=\"17\" y1=\"0\" x2=\"32\" y2=\"14\" />\n",
       "  <line x1=\"18\" y1=\"0\" x2=\"33\" y2=\"14\" />\n",
       "  <line x1=\"19\" y1=\"0\" x2=\"34\" y2=\"14\" />\n",
       "  <line x1=\"21\" y1=\"0\" x2=\"36\" y2=\"14\" />\n",
       "  <line x1=\"22\" y1=\"0\" x2=\"37\" y2=\"14\" />\n",
       "  <line x1=\"23\" y1=\"0\" x2=\"38\" y2=\"14\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"40\" y2=\"14\" />\n",
       "  <line x1=\"26\" y1=\"0\" x2=\"40\" y2=\"14\" />\n",
       "  <line x1=\"27\" y1=\"0\" x2=\"42\" y2=\"14\" />\n",
       "  <line x1=\"28\" y1=\"0\" x2=\"43\" y2=\"14\" />\n",
       "  <line x1=\"30\" y1=\"0\" x2=\"44\" y2=\"14\" />\n",
       "  <line x1=\"31\" y1=\"0\" x2=\"46\" y2=\"14\" />\n",
       "  <line x1=\"32\" y1=\"0\" x2=\"47\" y2=\"14\" />\n",
       "  <line x1=\"34\" y1=\"0\" x2=\"48\" y2=\"14\" />\n",
       "  <line x1=\"34\" y1=\"0\" x2=\"49\" y2=\"14\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"51\" y2=\"14\" />\n",
       "  <line x1=\"37\" y1=\"0\" x2=\"52\" y2=\"14\" />\n",
       "  <line x1=\"38\" y1=\"0\" x2=\"53\" y2=\"14\" />\n",
       "  <line x1=\"40\" y1=\"0\" x2=\"55\" y2=\"14\" />\n",
       "  <line x1=\"41\" y1=\"0\" x2=\"56\" y2=\"14\" />\n",
       "  <line x1=\"42\" y1=\"0\" x2=\"57\" y2=\"14\" />\n",
       "  <line x1=\"43\" y1=\"0\" x2=\"58\" y2=\"14\" />\n",
       "  <line x1=\"45\" y1=\"0\" x2=\"60\" y2=\"14\" />\n",
       "  <line x1=\"46\" y1=\"0\" x2=\"61\" y2=\"14\" />\n",
       "  <line x1=\"47\" y1=\"0\" x2=\"62\" y2=\"14\" />\n",
       "  <line x1=\"49\" y1=\"0\" x2=\"64\" y2=\"14\" />\n",
       "  <line x1=\"50\" y1=\"0\" x2=\"65\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 50.445289759298184,0.0 65.39388770905259,14.948597949754403 24.9485979497544,14.948597949754403\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"65\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"24\" y1=\"21\" x2=\"65\" y2=\"21\" />\n",
       "  <line x1=\"24\" y1=\"27\" x2=\"65\" y2=\"27\" />\n",
       "  <line x1=\"24\" y1=\"34\" x2=\"65\" y2=\"34\" />\n",
       "  <line x1=\"24\" y1=\"40\" x2=\"65\" y2=\"40\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"40\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"14\" x2=\"25\" y2=\"40\" />\n",
       "  <line x1=\"27\" y1=\"14\" x2=\"27\" y2=\"40\" />\n",
       "  <line x1=\"28\" y1=\"14\" x2=\"28\" y2=\"40\" />\n",
       "  <line x1=\"29\" y1=\"14\" x2=\"29\" y2=\"40\" />\n",
       "  <line x1=\"31\" y1=\"14\" x2=\"31\" y2=\"40\" />\n",
       "  <line x1=\"32\" y1=\"14\" x2=\"32\" y2=\"40\" />\n",
       "  <line x1=\"33\" y1=\"14\" x2=\"33\" y2=\"40\" />\n",
       "  <line x1=\"34\" y1=\"14\" x2=\"34\" y2=\"40\" />\n",
       "  <line x1=\"36\" y1=\"14\" x2=\"36\" y2=\"40\" />\n",
       "  <line x1=\"37\" y1=\"14\" x2=\"37\" y2=\"40\" />\n",
       "  <line x1=\"38\" y1=\"14\" x2=\"38\" y2=\"40\" />\n",
       "  <line x1=\"40\" y1=\"14\" x2=\"40\" y2=\"40\" />\n",
       "  <line x1=\"40\" y1=\"14\" x2=\"40\" y2=\"40\" />\n",
       "  <line x1=\"42\" y1=\"14\" x2=\"42\" y2=\"40\" />\n",
       "  <line x1=\"43\" y1=\"14\" x2=\"43\" y2=\"40\" />\n",
       "  <line x1=\"44\" y1=\"14\" x2=\"44\" y2=\"40\" />\n",
       "  <line x1=\"46\" y1=\"14\" x2=\"46\" y2=\"40\" />\n",
       "  <line x1=\"47\" y1=\"14\" x2=\"47\" y2=\"40\" />\n",
       "  <line x1=\"48\" y1=\"14\" x2=\"48\" y2=\"40\" />\n",
       "  <line x1=\"49\" y1=\"14\" x2=\"49\" y2=\"40\" />\n",
       "  <line x1=\"51\" y1=\"14\" x2=\"51\" y2=\"40\" />\n",
       "  <line x1=\"52\" y1=\"14\" x2=\"52\" y2=\"40\" />\n",
       "  <line x1=\"53\" y1=\"14\" x2=\"53\" y2=\"40\" />\n",
       "  <line x1=\"55\" y1=\"14\" x2=\"55\" y2=\"40\" />\n",
       "  <line x1=\"56\" y1=\"14\" x2=\"56\" y2=\"40\" />\n",
       "  <line x1=\"57\" y1=\"14\" x2=\"57\" y2=\"40\" />\n",
       "  <line x1=\"58\" y1=\"14\" x2=\"58\" y2=\"40\" />\n",
       "  <line x1=\"60\" y1=\"14\" x2=\"60\" y2=\"40\" />\n",
       "  <line x1=\"61\" y1=\"14\" x2=\"61\" y2=\"40\" />\n",
       "  <line x1=\"62\" y1=\"14\" x2=\"62\" y2=\"40\" />\n",
       "  <line x1=\"64\" y1=\"14\" x2=\"64\" y2=\"40\" />\n",
       "  <line x1=\"65\" y1=\"14\" x2=\"65\" y2=\"40\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"24.9485979497544,14.948597949754403 65.39388770905259,14.948597949754403 65.39388770905259,40.36121446433689 24.9485979497544,40.36121446433689\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"45.171243\" y=\"60.361214\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >91</text>\n",
       "  <text x=\"85.393888\" y=\"27.654906\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,85.393888,27.654906)\">4</text>\n",
       "  <text x=\"7.474299\" y=\"52.886915\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,7.474299,52.886915)\">3</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"135\" y1=\"0\" x2=\"157\" y2=\"22\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"135\" y1=\"90\" x2=\"157\" y2=\"112\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"135\" y1=\"0\" x2=\"135\" y2=\"90\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"157\" y1=\"22\" x2=\"157\" y2=\"112\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"135.0,0.0 157.99257868827482,22.992578688274808 157.99257868827482,112.9925786882748 135.0,90.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"135\" y1=\"0\" x2=\"255\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"157\" y1=\"22\" x2=\"277\" y2=\"22\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"135\" y1=\"0\" x2=\"157\" y2=\"22\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"255\" y1=\"0\" x2=\"277\" y2=\"22\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"135.0,0.0 255.0,0.0 277.9925786882748,22.992578688274808 157.99257868827482,22.992578688274808\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"157\" y1=\"22\" x2=\"277\" y2=\"22\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"157\" y1=\"112\" x2=\"277\" y2=\"112\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"157\" y1=\"22\" x2=\"157\" y2=\"112\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"277\" y1=\"22\" x2=\"277\" y2=\"112\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"157.99257868827482,22.992578688274808 277.9925786882748,22.992578688274808 277.9925786882748,112.9925786882748 157.99257868827482,112.9925786882748\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"217.992579\" y=\"132.992579\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >640</text>\n",
       "  <text x=\"297.992579\" y=\"67.992579\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,297.992579,67.992579)\">480</text>\n",
       "  <text x=\"136.496289\" y=\"121.496289\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,136.496289,121.496289)\">70</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(3, 4, 91, 70, 480, 640), dtype=float32, chunksize=(1, 1, 1, 70, 480, 640), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Show array storage metadata:\")\n",
    "inp_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8415a961-6b0e-41f8-804b-9a575ff315c3",
   "metadata": {},
   "source": [
    "### Flatten the arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee5cb9",
   "metadata": {},
   "source": [
    "Flatten time and lat/long down to a single dimension, sample number </br>\n",
    "Function expects 6-d array where each expected dimension is named in the function - cube_num, time, time2, height, lat, long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "851b2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_cubes_with_numpy(np_array):\n",
    "\n",
    "    # print('input dimensions:', np_array.shape)\n",
    "\n",
    "    cube_num, time, time2, height, lat, long = np_array.shape\n",
    "\n",
    "    # # verify shape\n",
    "    # print(np_array.shape)\n",
    "\n",
    "    # swap axis of time and height to ensure flattening preserves height\n",
    "    cube_array = np_array.transpose(0, 3, 1, 2, 4, 5)\n",
    "    cubes_flattened = np.reshape(\n",
    "        cube_array, (cube_num, height, (lat * long * time * time2))\n",
    "    )\n",
    "\n",
    "    # print('new dimensions', cubes_flattened.shape)\n",
    "\n",
    "    cube_to_return = cubes_flattened.T\n",
    "    # remove unnecessary dimensions\n",
    "    cube_to_return = cube_to_return.squeeze()\n",
    "    return cube_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9feddf51-b706-4221-8e61-0e13a31612a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of flattened and transposed arrays:\n",
      "Input: (111820800, 70, 3)\n",
      "Target: (111820800, 70)\n"
     ]
    }
   ],
   "source": [
    "dask.config.set(\n",
    "    {\"array.slicing.split_large_chunks\": False}\n",
    ")  # allow the potentially large chunk of data\n",
    "\n",
    "inp_array = flatten_cubes_with_numpy(inp_array)\n",
    "tar_array = flatten_cubes_with_numpy(tar_array)\n",
    "\n",
    "# print('verify squeeze')\n",
    "print(\"Shapes of flattened and transposed arrays:\")\n",
    "print(\"Input:\", inp_array.shape)\n",
    "print(\"Target:\", tar_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a621d-3cce-471b-95b9-48d7fd540b69",
   "metadata": {},
   "source": [
    "Rechunk large data to ensure large chunks are reduced for easier handling in dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9d12c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rechunked array storage metadata for target:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 29.16 GiB </td>\n",
       "                        <td> 106.64 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (111820800, 70) </td>\n",
       "                        <td> (3993600, 7) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 12 Graph Layers </td>\n",
       "                        <td> 280 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float32 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"4\" x2=\"25\" y2=\"4\" />\n",
       "  <line x1=\"0\" y1=\"8\" x2=\"25\" y2=\"8\" />\n",
       "  <line x1=\"0\" y1=\"17\" x2=\"25\" y2=\"17\" />\n",
       "  <line x1=\"0\" y1=\"21\" x2=\"25\" y2=\"21\" />\n",
       "  <line x1=\"0\" y1=\"30\" x2=\"25\" y2=\"30\" />\n",
       "  <line x1=\"0\" y1=\"34\" x2=\"25\" y2=\"34\" />\n",
       "  <line x1=\"0\" y1=\"42\" x2=\"25\" y2=\"42\" />\n",
       "  <line x1=\"0\" y1=\"47\" x2=\"25\" y2=\"47\" />\n",
       "  <line x1=\"0\" y1=\"55\" x2=\"25\" y2=\"55\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"25\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"68\" x2=\"25\" y2=\"68\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"25\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"81\" x2=\"25\" y2=\"81\" />\n",
       "  <line x1=\"0\" y1=\"85\" x2=\"25\" y2=\"85\" />\n",
       "  <line x1=\"0\" y1=\"94\" x2=\"25\" y2=\"94\" />\n",
       "  <line x1=\"0\" y1=\"98\" x2=\"25\" y2=\"98\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"111\" x2=\"25\" y2=\"111\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"2\" y1=\"0\" x2=\"2\" y2=\"120\" />\n",
       "  <line x1=\"5\" y1=\"0\" x2=\"5\" y2=\"120\" />\n",
       "  <line x1=\"7\" y1=\"0\" x2=\"7\" y2=\"120\" />\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"120\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"120\" />\n",
       "  <line x1=\"15\" y1=\"0\" x2=\"15\" y2=\"120\" />\n",
       "  <line x1=\"17\" y1=\"0\" x2=\"17\" y2=\"120\" />\n",
       "  <line x1=\"20\" y1=\"0\" x2=\"20\" y2=\"120\" />\n",
       "  <line x1=\"22\" y1=\"0\" x2=\"22\" y2=\"120\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >70</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">111820800</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<rechunk-merge, shape=(111820800, 70), dtype=float32, chunksize=(3993600, 7), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_array = dask.array.rechunk(tar_array, chunks=\"auto\")\n",
    "print(\"Rechunked array storage metadata for target:\")\n",
    "tar_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb3a68d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rechunked array storage metadata for input:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 87.48 GiB </td>\n",
       "                        <td> 124.41 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (111820800, 70, 3) </td>\n",
       "                        <td> (4659200, 7, 1) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 24 Graph Layers </td>\n",
       "                        <td> 720 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float32 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"156\" height=\"146\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"2\" x2=\"80\" y2=\"73\" />\n",
       "  <line x1=\"10\" y1=\"5\" x2=\"80\" y2=\"75\" />\n",
       "  <line x1=\"10\" y1=\"7\" x2=\"80\" y2=\"78\" />\n",
       "  <line x1=\"10\" y1=\"10\" x2=\"80\" y2=\"80\" />\n",
       "  <line x1=\"10\" y1=\"12\" x2=\"80\" y2=\"83\" />\n",
       "  <line x1=\"10\" y1=\"15\" x2=\"80\" y2=\"85\" />\n",
       "  <line x1=\"10\" y1=\"17\" x2=\"80\" y2=\"88\" />\n",
       "  <line x1=\"10\" y1=\"20\" x2=\"80\" y2=\"90\" />\n",
       "  <line x1=\"10\" y1=\"22\" x2=\"80\" y2=\"93\" />\n",
       "  <line x1=\"10\" y1=\"25\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"12\" y2=\"28\" />\n",
       "  <line x1=\"15\" y1=\"5\" x2=\"15\" y2=\"31\" />\n",
       "  <line x1=\"18\" y1=\"8\" x2=\"18\" y2=\"34\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"40\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"43\" />\n",
       "  <line x1=\"30\" y1=\"20\" x2=\"30\" y2=\"46\" />\n",
       "  <line x1=\"33\" y1=\"23\" x2=\"33\" y2=\"48\" />\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"39\" y2=\"54\" />\n",
       "  <line x1=\"42\" y1=\"32\" x2=\"42\" y2=\"57\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"45\" y2=\"60\" />\n",
       "  <line x1=\"48\" y1=\"38\" x2=\"48\" y2=\"63\" />\n",
       "  <line x1=\"54\" y1=\"44\" x2=\"54\" y2=\"69\" />\n",
       "  <line x1=\"57\" y1=\"47\" x2=\"57\" y2=\"72\" />\n",
       "  <line x1=\"60\" y1=\"50\" x2=\"60\" y2=\"75\" />\n",
       "  <line x1=\"62\" y1=\"52\" x2=\"62\" y2=\"78\" />\n",
       "  <line x1=\"68\" y1=\"58\" x2=\"68\" y2=\"84\" />\n",
       "  <line x1=\"71\" y1=\"61\" x2=\"71\" y2=\"87\" />\n",
       "  <line x1=\"74\" y1=\"64\" x2=\"74\" y2=\"90\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,96.00085180870013 10.0,25.412616514582485\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"38\" y2=\"2\" />\n",
       "  <line x1=\"15\" y1=\"5\" x2=\"41\" y2=\"5\" />\n",
       "  <line x1=\"18\" y1=\"8\" x2=\"44\" y2=\"8\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"50\" y2=\"14\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"53\" y2=\"17\" />\n",
       "  <line x1=\"30\" y1=\"20\" x2=\"56\" y2=\"20\" />\n",
       "  <line x1=\"33\" y1=\"23\" x2=\"58\" y2=\"23\" />\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"64\" y2=\"29\" />\n",
       "  <line x1=\"42\" y1=\"32\" x2=\"67\" y2=\"32\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"70\" y2=\"35\" />\n",
       "  <line x1=\"48\" y1=\"38\" x2=\"73\" y2=\"38\" />\n",
       "  <line x1=\"54\" y1=\"44\" x2=\"79\" y2=\"44\" />\n",
       "  <line x1=\"57\" y1=\"47\" x2=\"82\" y2=\"47\" />\n",
       "  <line x1=\"60\" y1=\"50\" x2=\"85\" y2=\"50\" />\n",
       "  <line x1=\"62\" y1=\"52\" x2=\"88\" y2=\"52\" />\n",
       "  <line x1=\"68\" y1=\"58\" x2=\"94\" y2=\"58\" />\n",
       "  <line x1=\"71\" y1=\"61\" x2=\"97\" y2=\"61\" />\n",
       "  <line x1=\"74\" y1=\"64\" x2=\"100\" y2=\"64\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"18\" y1=\"0\" x2=\"89\" y2=\"70\" />\n",
       "  <line x1=\"26\" y1=\"0\" x2=\"97\" y2=\"70\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 35.41261651458248,0.0 106.00085180870013,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"73\" x2=\"106\" y2=\"73\" />\n",
       "  <line x1=\"80\" y1=\"75\" x2=\"106\" y2=\"75\" />\n",
       "  <line x1=\"80\" y1=\"78\" x2=\"106\" y2=\"78\" />\n",
       "  <line x1=\"80\" y1=\"80\" x2=\"106\" y2=\"80\" />\n",
       "  <line x1=\"80\" y1=\"83\" x2=\"106\" y2=\"83\" />\n",
       "  <line x1=\"80\" y1=\"85\" x2=\"106\" y2=\"85\" />\n",
       "  <line x1=\"80\" y1=\"88\" x2=\"106\" y2=\"88\" />\n",
       "  <line x1=\"80\" y1=\"90\" x2=\"106\" y2=\"90\" />\n",
       "  <line x1=\"80\" y1=\"93\" x2=\"106\" y2=\"93\" />\n",
       "  <line x1=\"80\" y1=\"96\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"89\" y1=\"70\" x2=\"89\" y2=\"96\" />\n",
       "  <line x1=\"97\" y1=\"70\" x2=\"97\" y2=\"96\" />\n",
       "  <line x1=\"106\" y1=\"70\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 106.00085180870013,70.58823529411765 106.00085180870013,96.00085180870013 80.58823529411765,96.00085180870013\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"93.294544\" y=\"116.000852\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n",
       "  <text x=\"126.000852\" y=\"83.294544\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,126.000852,83.294544)\">70</text>\n",
       "  <text x=\"35.294118\" y=\"80.706734\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,80.706734)\">111820800</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<rechunk-merge, shape=(111820800, 70, 3), dtype=float32, chunksize=(4659200, 7, 1), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_array = dask.array.rechunk(inp_array, chunks=\"auto\")\n",
    "print(\"Rechunked array storage metadata for input:\")\n",
    "inp_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddfa4f-34d0-4f37-b7c6-3f402880b21e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess the data toward ML algorithm input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60364b-a938-487f-b630-2f740d02ed50",
   "metadata": {},
   "source": [
    "### Generate data for the target of cloud base at certain height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f750de3",
   "metadata": {},
   "source": [
    "preprocess the target\n",
    "for the target, we define a cloud existing in a height layer:\n",
    "if the cloud volume fraction is greater than 2 out of possible 8 oktas </br>\n",
    "the cell below finds the first occurrences where the cloud volume is greater than the threshold marking a 1 in the array location, and stores 0 otherwise. </br>\n",
    "Later, the final height layer will be marker for samples without a cloud base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef513328-caa2-400a-aecf-d1ec119f5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_threshold = 2.0 / 8.0\n",
    "cloud_over_threshold = dask.array.where(tar_array > cloud_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e54c4b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start base found sample compute\n",
      "Start sample index compute\n",
      "CPU times: user 394 ms, sys: 474 ms, total: 868 ms\n",
      "Wall time: 656 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# realize the values for the where condition (dask array to numpy array)\n",
    "print(\"Start base found sample compute\")\n",
    "sample_with_cloud = cloud_over_threshold[0].compute()\n",
    "print(\"Start sample index compute\")\n",
    "index_on_sample = cloud_over_threshold[1].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d978cc26-a794-46eb-ba5d-40bf231e8eeb",
   "metadata": {},
   "source": [
    "Remove repeat indicies, e.g. where there are multiple layers above the cloud threshold, we only want the first occurence in a sample (the base):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4a7bfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.3 ms, sys: 34.1 ms, total: 62.4 ms\n",
      "Wall time: 60.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_, first_duplicate_indicies = np.unique(sample_with_cloud, return_index=True)\n",
    "\n",
    "if COMPUTE_CLOUD_BASE_SAMPLE_NUMBER:\n",
    "    print(\"Start duplicate indicies compute\")\n",
    "    first_duplicate_indicies = first_duplicate_indicies.compute()\n",
    "    print(\"Number of cloud bases found:\", first_duplicate_indicies.shape)\n",
    "    print(\"Out of samples:\", tar_array.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5a79c",
   "metadata": {},
   "source": [
    "For clouds where no base was found, add a marker at the final height layer\n",
    "(where no cloud volume over threshold appears in the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68006d8c-1fd4-45ee-bf6b-d41062687915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 102 ms, sys: 68.3 ms, total: 171 ms\n",
      "Wall time: 173 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# encode the cloud in onehot vector\n",
    "one_hot_encoded_bases = np.zeros(tar_array.shape)\n",
    "one_hot_encoded_bases[\n",
    "    sample_with_cloud[first_duplicate_indicies],\n",
    "    index_on_sample[first_duplicate_indicies],\n",
    "] = 1\n",
    "# mark the end (final layer) if no cloud base detected\n",
    "flip = lambda booleanVal: not booleanVal\n",
    "vflip = np.vectorize(flip)\n",
    "one_hot_encoded_bases[np.where(vflip(np.any(one_hot_encoded_bases, axis=1)))[0], -1] = 1\n",
    "\n",
    "# Now reduce vectors as if each height layer is treated as a class where the model will predict, onehot -> class label e.g. 0,0,1,0, -> 2\n",
    "class_label_encoded_bases = np.argmax(one_hot_encoded_bases, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4a8db50-97af-4e20-acd8-b145ca71e31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target as class label: (307200,)\n",
      "Output dim: (307200, 70)\n"
     ]
    }
   ],
   "source": [
    "print(\"Target as class label:\", class_label_encoded_bases.shape)\n",
    "print(\"Output dim:\", one_hot_encoded_bases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba8546d5-6bf3-4faf-97c3-70f0f7ef92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, free up some memory\n",
    "if FREE_UP_MEMORY_AFTER_TARGET_COMPUTATION:\n",
    "    del sample_with_cloud\n",
    "    del cloud_over_threshold\n",
    "    del first_duplicate_indicies\n",
    "    del index_on_sample\n",
    "    del tar_dict\n",
    "    del tar_cube\n",
    "    del cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30307449-f82b-4082-8c31-47f0768631d3",
   "metadata": {},
   "source": [
    "### Normalize input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8f418",
   "metadata": {},
   "source": [
    "For the normalization of input data: we first transpose the input array so that the feature dimension is at the top level of the array, and numpy has an easier time accessing all values of the same feature. Then all values are normalized by being scaled in the range \\[0,1\\]\n",
    "\n",
    "(must investigate mistake relating to ptp of local datasets instead of global values and make changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fe624c8-e933-4cf8-b8a0-ba927853b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished compute of input array normalization\n",
      "Number of masked values after computation: 0\n",
      "CPU times: user 1.14 s, sys: 1.2 s, total: 2.34 s\n",
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inp_array = inp_array.T\n",
    "inp_array = (inp_array - np.min(inp_array, axis=(1, 2)).reshape((3, 1, 1))) / (\n",
    "    np.ptp(inp_array, axis=(1, 2)).reshape((3, 1, 1))\n",
    ")\n",
    "inp_array = inp_array.T\n",
    "\n",
    "# a 2 half compute used to avoid memory constraints\n",
    "if COMPUTE_INPUT_ARRAY_IN_HALVES:\n",
    "    half = int(len(inp_array) / 2)\n",
    "    inp_array_1 = inp_array[:half].compute()\n",
    "    len_first_half = inp_array_1.shape[0]\n",
    "\n",
    "else:\n",
    "    inp_array = inp_array.compute()\n",
    "    print(\"Finished compute of input array normalization\")\n",
    "    # convert to regular array, after verifying mask does not identify any values\n",
    "    # (print below gives 0 masked values)\n",
    "    num_of_masked = np.ma.count_masked(inp_array)\n",
    "    print(\"Number of masked values after computation:\", num_of_masked)\n",
    "    assert num_of_masked == 0\n",
    "    # unmask, giving all masked values NaN (but no masked values)\n",
    "    inp_array = np.ma.filled(inp_array, np.nan)\n",
    "\n",
    "    # # verify dimensions\n",
    "    # print(inp_array.shape)\n",
    "    # # and verify type\n",
    "    # print('type of unmasked array:', type(inp_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "945e3063-1795-4b04-9a3e-717b29b943df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 s, sys: 1e+03 ns, total: 3 s\n",
      "Wall time: 5.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# second half of memory constraint compute, see above cell\n",
    "if COMPUTE_INPUT_ARRAY_IN_HALVES:\n",
    "    inp_array_2 = inp_array[half:].compute()\n",
    "    print(\"Array type after compute:\", type(inp_array_2))\n",
    "    num_of_masked = np.ma.count_masked(inp_array_2)\n",
    "    print(\"Count of masked (unfilled) values:\", num_of_masked)\n",
    "    assert num_of_masked == 0\n",
    "    inp_array_2 = np.ma.filled(inp_array_2, np.nan)\n",
    "    print(\"Array type after compute:\", type(inp_array_2))\n",
    "    len_second_half = inp_array_2.shape[0]\n",
    "\n",
    "    # verify\n",
    "    print(len_second_half + len_first_half)\n",
    "    print(inp_array.shape)\n",
    "\n",
    "    # combine halves\n",
    "    inp_array = inp_array_1 = np.concatenate((inp_array_1, inp_array_2), axis=0)\n",
    "    del inp_array_1\n",
    "    del inp_array_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a4ea3-c4d8-4d2c-a53e-0c4c99761374",
   "metadata": {},
   "source": [
    "Compute and unmask target array (cloud volume):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fdf90f4-48e1-45bb-8187-3194ec86a534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current type of target array: <class 'dask.array.core.Array'>\n",
      "Target shape: (307200, 70)\n",
      "Finished compute of target array\n",
      "Number of masked values after computation: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Current type of target array:\", type(tar_array))\n",
    "print(\"Target shape:\", tar_array.shape)\n",
    "tar_array = tar_array.compute()\n",
    "print(\"Finished compute of target array\")\n",
    "\n",
    "num_of_masked = np.ma.count_masked(tar_array)\n",
    "print(\"Number of masked values after computation:\", num_of_masked)\n",
    "assert num_of_masked == 0\n",
    "\n",
    "# unmask\n",
    "tar_array = np.ma.filled(tar_array, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "548d7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VERIFY_NO_FINAL_LAYER_CLOUDS:\n",
    "    # verify the claim that no cloud bases appear in the final layer\n",
    "    # can be strengthened to, no clouds exist in the final layer (next line returns 0)\n",
    "    print(\n",
    "        \"list of clouds at final height level:\",\n",
    "        np.where(tar_array[:, -1] > cloud_threshold),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807eee2f",
   "metadata": {},
   "source": [
    "#### View the produced arrays which are ready to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7717fbcd-bd13-4580-9923-38d254344485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.6558385 , 0.43776825, 0.00130421],\n",
       "        [0.6535534 , 0.44206008, 0.00150347],\n",
       "        [0.6503736 , 0.4434907 , 0.00167781],\n",
       "        [0.6463207 , 0.45422032, 0.002065  ],\n",
       "        [0.64145505, 0.47353363, 0.00277824]],\n",
       "\n",
       "       [[0.6559205 , 0.43776825, 0.00130421],\n",
       "        [0.6536282 , 0.44563663, 0.00151932],\n",
       "        [0.65044475, 0.45064378, 0.00176159],\n",
       "        [0.64639425, 0.4620887 , 0.00215331],\n",
       "        [0.6415262 , 0.48211733, 0.00282579]],\n",
       "\n",
       "       [[0.6559844 , 0.43776825, 0.00132006],\n",
       "        [0.6536885 , 0.4434907 , 0.00148988],\n",
       "        [0.6505026 , 0.45064378, 0.00176159],\n",
       "        [0.6464521 , 0.4620887 , 0.0021601 ],\n",
       "        [0.6415829 , 0.481402  , 0.00282353]],\n",
       "\n",
       "       [[0.65603024, 0.43705294, 0.00132459],\n",
       "        [0.6537331 , 0.44206008, 0.00148082],\n",
       "        [0.6505448 , 0.44992846, 0.00175027],\n",
       "        [0.64649314, 0.4620887 , 0.00216237],\n",
       "        [0.6416239 , 0.4806867 , 0.00282353]],\n",
       "\n",
       "       [[0.6560628 , 0.43776825, 0.00133138],\n",
       "        [0.6537656 , 0.44206008, 0.0014763 ],\n",
       "        [0.6505762 , 0.44921315, 0.00174121],\n",
       "        [0.6465221 , 0.4620887 , 0.00216237],\n",
       "        [0.6416528 , 0.4806867 , 0.00282353]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show 5 samples, first 5 height layers only, displaying all features in the layer (features not indexed)\n",
    "# (automatic numpy array display reduction is quite large for this array)\n",
    "inp_array[0:5, 0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8db57938-d1b2-4539-981d-2a4fae24a01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 15, 15, ...,  0,  0,  0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_label_encoded_bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd93c9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.140625, 0.125   , 0.09375 , ..., 0.      , 0.      , 0.      ],\n",
       "       [0.140625, 0.109375, 0.09375 , ..., 0.      , 0.      , 0.      ],\n",
       "       [0.140625, 0.125   , 0.09375 , ..., 0.      , 0.      , 0.      ],\n",
       "       ...,\n",
       "       [0.90625 , 1.      , 1.      , ..., 0.      , 0.      , 0.      ],\n",
       "       [1.      , 1.      , 1.      , ..., 0.      , 0.      , 0.      ],\n",
       "       [0.953125, 1.      , 1.      , ..., 0.      , 0.      , 0.      ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b371da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded_bases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f14d59",
   "metadata": {},
   "source": [
    "#### Save a selection of wanted arrays (inp_array, tar_array, one_hot_encoded_bases)\n",
    "\n",
    "Now to save the computed array </br>\n",
    "(will not save one of class label output or one_hot as easy conversion between the two)</br>\n",
    "(went with saving one-hot to emulate the data produced/used by base solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c385de7a-6c99-443a-b793-3cef3ff39dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dim: (307200, 70, 3)\n",
      "Cloud Output dim: (307200, 70)\n"
     ]
    }
   ],
   "source": [
    "# verify input and output shapes\n",
    "print(\"Input dim:\", inp_array.shape)\n",
    "print(\"Cloud Output dim:\", tar_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05c40478-6374-40fa-93f7-4ad4fc71ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving numpy arrays\n",
      "CPU times: user 694 ms, sys: 155 ms, total: 848 ms\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Saving numpy arrays\")\n",
    "\n",
    "with open(path_to_save_result, \"w+b\") as f:\n",
    "    # variable assignment that name the arrays for the saved file\n",
    "    input_x = inp_array\n",
    "    output_onehot = one_hot_encoded_bases\n",
    "    output_cloud_volume = tar_array\n",
    "\n",
    "    np.savez(\n",
    "        f,\n",
    "        input_x=input_x,\n",
    "        output_cloud_volume=output_cloud_volume,\n",
    "        output_onehot=output_onehot,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178079d5",
   "metadata": {},
   "source": [
    "The following cell is code to create a positional encoding for the height layers in the data, e.g. the data at height layer 0 would have the positional encoding of: 0 as part of the input feature. It is commented out as PyTorch Dataloaders are found to have the capability to produce this information at load-time, which seems like a better option than creating a potentially huge array for each position that is scaled up to the size of the sample number redundantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f09d7a7-82bb-4163-9eda-55ea7ab25730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an extra positional encoding optionally for input use\n",
    "if GENERATE_POSITIONAL_ENCODING_ARRAYS:\n",
    "    sample_num, height_dim, _ = inp_array.shape\n",
    "    # generate height values\n",
    "    height_position_vector = np.arange(height_dim)\n",
    "    # extend dimensions out to match input feats\n",
    "    height_position_vector = np.repeat([height_position_vector], sample_num, axis=0)\n",
    "\n",
    "    # verify\n",
    "    print(\"shape of encoding vector:\", height_position_vector.shape)\n",
    "\n",
    "    x, y = height_position_vector.shape\n",
    "    # add a dimension for height to act as a feature\n",
    "    height_position_vector = height_position_vector.reshape(x, y, 1)\n",
    "\n",
    "    # fit the dtype of the feature to match the dtype of other feats\n",
    "    height_position_vector = height_position_vector.astype(inp_array.dtype)\n",
    "\n",
    "    # combine height feature into input array\n",
    "    if CONCATENATE_POSITIONAL_ENCONDING_TO_FEATURE_VECTOR:\n",
    "        inp_array = np.concatenate(\n",
    "            (height_position_vector, inp_array), axis=2, dtype=np.float32\n",
    "        )  # leave the concat for within the model after producing embedding\n",
    "\n",
    "    # verify datatypes\n",
    "    print(\"input dtype\", inp_array.dtype)\n",
    "    print(\"height encoding dtype\", height_position_vector.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd290a-62fc-4135-943f-4dd10e3ac7ac",
   "metadata": {},
   "source": [
    "## Convert saved numpy arrays to zarr files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cab18d-3cb2-44d4-9535-b658428d1b4f",
   "metadata": {},
   "source": [
    "First, open the numpy files and load each array into a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc554c7a-5a8a-431b-9fb2-de5aee1eedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr  # (package not in cop torch conda env)\n",
    "\n",
    "with open(path_to_save_result, \"r+b\") as f:\n",
    "    numpy_files = np.load(f)\n",
    "    inp_arr = numpy_files[\"input_x\"]\n",
    "    onehot_arr = numpy_files[\"output_onehot\"]\n",
    "    tar_arr = numpy_files[\"output_cloud_volume\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505f30c-e5ca-4a36-ae52-b9e107888d3f",
   "metadata": {},
   "source": [
    "Then, store the loaded numpy arrays into zarr, which will chunk and compress each array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69cdc387-0bb8-46fa-96ba-28288a8cf035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of zarr group:\n",
      "<zarr.core.Array '/cloud_volume_fraction_y.zarr' (307200, 70) float32>\n",
      "<zarr.core.Array '/humidity_temp_pressure_x.zarr' (307200, 70, 3) float32>\n",
      "<zarr.core.Array '/onehot_cloud_base_height_y.zarr' (307200, 70) float64>\n",
      "\n",
      "Tree of zarr group:\n",
      " /\n",
      "  cloud_volume_fraction_y.zarr (307200, 70) float32\n",
      "  humidity_temp_pressure_x.zarr (307200, 70, 3) float32\n",
      "  onehot_cloud_base_height_y.zarr (307200, 70) float64\n",
      "\n",
      "Shape array example: (307200, 70, 3)\n",
      "\n",
      "Zarr chunking shape of the array: (38400, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "store = zarr.DirectoryStore(path_to_save_zarr)\n",
    "# define objected for arrays to be grouped under\n",
    "\n",
    "zarr_grouping = zarr.group(store=store, overwrite=True)\n",
    "\n",
    "# initialize and then write on zarr arrays for all desired arrays to be saved\n",
    "\n",
    "humidity_temp_pressure_x = zarr_grouping.zeros(\n",
    "    shape=inp_arr.shape, dtype=inp_arr.dtype, name=\"humidity_temp_pressure_x.zarr\"\n",
    ")\n",
    "humidity_temp_pressure_x[:] = inp_arr\n",
    "\n",
    "onehot_cloud_base_height_y = zarr_grouping.zeros(\n",
    "    shape=onehot_arr.shape,\n",
    "    dtype=onehot_arr.dtype,\n",
    "    name=\"onehot_cloud_base_height_y.zarr\",\n",
    ")\n",
    "onehot_cloud_base_height_y[:] = onehot_arr\n",
    "\n",
    "cloud_volume_fraction_y = zarr_grouping.zeros(\n",
    "    shape=tar_arr.shape, dtype=tar_arr.dtype, name=\"cloud_volume_fraction_y.zarr\"\n",
    ")\n",
    "cloud_volume_fraction_y[:] = tar_arr\n",
    "\n",
    "# output some summary for zarr\n",
    "# view group values\n",
    "printF = lambda obj: print(obj)\n",
    "print(\"Elements of zarr group:\")\n",
    "zarr_grouping.visitvalues(printF)\n",
    "# view group tree\n",
    "print(\"\\nTree of zarr group:\\n\", zarr_grouping.tree())\n",
    "# see chunk size\n",
    "print(\"\\nShape array example:\", humidity_temp_pressure_x.shape)\n",
    "print(\"\\nZarr chunking shape of the array:\", humidity_temp_pressure_x.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7255e5c4-dbf5-40dd-aaf3-2d68d5172d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/hsouth/cbh_data/analysis_ready/example_train.zarr\n",
      "Loaded zarr, file information:\n",
      " Name        : /\n",
      "Type        : zarr.hierarchy.Group\n",
      "Read-only   : False\n",
      "Store type  : zarr.storage.DirectoryStore\n",
      "No. members : 3\n",
      "No. arrays  : 3\n",
      "No. groups  : 0\n",
      "Arrays      : cloud_volume_fraction_y.zarr, humidity_temp_pressure_x.zarr,\n",
      "            : onehot_cloud_base_height_y.zarr\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, verify that a load back in of the data preserves desired qualities\n",
    "print(path_to_save_zarr)\n",
    "x, lab, y = cbh_data_definitions.load_data_from_zarr('/scratch/hsouth/cbh_data/analysis_ready/train.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4bebbf4e-6561-40cb-8a84-0e101c134de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.015625 0.015625 0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.      ]\n",
      "(array([], dtype=int64),)\n",
      "vol_base 69\n",
      "base_label_position 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "('base mismatch', 69, 'vs', 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvol_base\u001b[39m\u001b[38;5;124m'\u001b[39m, vol_base)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_label_position\u001b[39m\u001b[38;5;124m'\u001b[39m, base_label_position)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m vol_base \u001b[38;5;241m==\u001b[39m base_label_position, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase mismatch\u001b[39m\u001b[38;5;124m'\u001b[39m, vol_base, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvs\u001b[39m\u001b[38;5;124m'\u001b[39m, base_label_position)\n",
      "\u001b[0;31mAssertionError\u001b[0m: ('base mismatch', 69, 'vs', 0)"
     ]
    }
   ],
   "source": [
    "# Do the samples match up across groups?\n",
    "threshold = 2.0/8.0\n",
    "\n",
    "# same sample number\n",
    "assert len(x) == len(y) == len(lab)\n",
    "# preserved order (checked with between label and volume comparison\n",
    "one_percent_selection = int(0.01*len(x))\n",
    "indices_to_test = np.random.choice(np.arange(len(x)), size=one_percent_selection)\n",
    "for i in range(len(indices_to_test)):\n",
    "    vol = y[indices_to_test[i]].compute()\n",
    "    base = lab[indices_to_test[i]].compute()\n",
    "    base_label_position = np.argmax(base)\n",
    "    print(vol)\n",
    "    thresh_overcome = np.where(vol >= threshold)\n",
    "    \n",
    "    print(thresh_overcome)\n",
    "    try:\n",
    "        vol_base = thresh_overcome[0][0]\n",
    "    except:\n",
    "        vol_base = len(vol) - 1\n",
    "    print('vol_base', vol_base)\n",
    "    print('base_label_position', base_label_position)\n",
    "    assert vol_base == base_label_position, ('base mismatch', vol_base, 'vs', base_label_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1b7c9-ba1f-4756-8433-5086219f5237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
